{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "from timeit import default_timer as timer\n",
    "import mock\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "from keras import backend as K\n",
    "from tensorflow.python.util import nest\n",
    "from keras.utils import np_utils\n",
    "\n",
    "f = np.load('mnist.npz')\n",
    "X_train, y_train = f['x_train'], f['y_train']\n",
    "X_test, y_test = f['x_test'], f['y_test']\n",
    "f.close()\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "n_epoch = 70\n",
    "num_steps = 100\n",
    "evaluation_period = 10\n",
    "logging_period = 10\n",
    "evaluation_epochs = 20\n",
    "batch_size = 128\n",
    "num_dims = 10\n",
    "num_layer = 2\n",
    "layers = 20\n",
    "hidden_size = 20\n",
    "unroll_nn = 20\n",
    "lr = 0.001\n",
    "logs_path = '/Users/kalyanb/PycharmProjects/Final-Code/MetaLog1/'\n",
    "save_path = '/Users/kalyanb/PycharmProjects/Final-Code/MetaOpt1/model.ckpt'\n",
    "alpha = 0.1\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def log_encode(x, p=10.0):\n",
    "    xa = tf.log(tf.maximum(tf.abs(x), np.exp(-p))) / p\n",
    "    xb = tf.clip_by_value(x * np.exp(p), -1, 1)\n",
    "    return tf.stack([xa, xb], axis=1)\n",
    "\n",
    "def _wrap_variable_creation(func, custom_getter):\n",
    "    \"\"\"Provides a custom getter for all variable creations.\"\"\"\n",
    "    original_get_variable = tf.get_variable\n",
    "    def custom_get_variable(*args, **kwargs):\n",
    "        if hasattr(kwargs, \"custom_getter\"):\n",
    "            raise AttributeError(\"Custom getters are not supported for optimizee variables.\")\n",
    "        return original_get_variable(*args, custom_getter=custom_getter, **kwargs)\n",
    "\n",
    "  # Mock the get_variable method.\n",
    "    with mock.patch(\"tensorflow.get_variable\", custom_get_variable):\n",
    "        return func()\n",
    "\n",
    "\n",
    "def _get_variables(func):\n",
    "    variables = []\n",
    "    constants = []\n",
    "\n",
    "    def custom_getter(getter, name, **kwargs):\n",
    "        trainable = kwargs[\"trainable\"]\n",
    "        kwargs[\"trainable\"] = False\n",
    "        variable = getter(name, **kwargs)\n",
    "        if trainable:\n",
    "            variables.append(variable)\n",
    "        else:\n",
    "            constants.append(variable)\n",
    "        return variable\n",
    "\n",
    "    _wrap_variable_creation(func, custom_getter)\n",
    "\n",
    "    return variables, constants\n",
    "\n",
    "\n",
    "def _make_with_custom_variables(func, variables):\n",
    "    variables = collections.deque(variables)\n",
    "\n",
    "    def custom_getter(getter, name, **kwargs):\n",
    "        if kwargs[\"trainable\"]:\n",
    "            return variables.popleft()\n",
    "        else:\n",
    "            kwargs[\"reuse\"] = True\n",
    "            return getter(name, **kwargs)\n",
    "    return _wrap_variable_creation(func, custom_getter)\n",
    "\n",
    "def problem(mode = \"train\"):\n",
    "    if mode == \"train\":\n",
    "        X = X_train / 255\n",
    "        y = np_utils.to_categorical(y_train)\n",
    "    else:\n",
    "        X = X_test / 255\n",
    "        y = np_utils.to_categorical(y_test)\n",
    "    images = tf.constant(X, dtype=tf.float32)\n",
    "    labels = tf.constant(y, dtype=tf.int64)\n",
    "\n",
    "    with tf.name_scope('Optimizee_loss'):\n",
    "        def compute_loss():\n",
    "            indices = tf.random_uniform([batch_size], 0, images.shape[0], tf.int64)\n",
    "            batch_images = tf.gather(images, indices)\n",
    "            batch_labels = tf.gather(labels, indices)\n",
    "            with tf.variable_scope(\"MLP\",reuse=tf.AUTO_REUSE):\n",
    "                W_in = tf.get_variable(\"x1\",\n",
    "                                        shape=[batch_images.shape[1], layers],\n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "                b_in = tf.get_variable(\"x2\",\n",
    "                                        shape=[layers, ],\n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "                W_out = tf.get_variable(\"x3\",\n",
    "                                        shape = [layers, batch_labels.shape[1]],\n",
    "                                        dtype = tf.float32,\n",
    "                                        initializer = tf.random_normal_initializer(stddev=0.01))\n",
    "                b_out = tf.get_variable(\"x4\",\n",
    "                                        shape=[batch_labels.shape[1], ],\n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "            layer_out = tf.sigmoid(tf.add(tf.matmul(batch_images, W_in), b_in))\n",
    "            output = tf.add(tf.matmul(layer_out, W_out), b_out)\n",
    "            return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output,\n",
    "                                                        labels=batch_labels))\n",
    "    return compute_loss\n",
    "\n",
    "def metaopti(loss):\n",
    "    opt_var = _get_variables(loss)[0]\n",
    "    shapes = [K.get_variable_shape(p) for p in opt_var]\n",
    "    with tf.variable_scope(\"softmax\",reuse=tf.AUTO_REUSE):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", shape=[hidden_size, 1], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", shape=[1], dtype=tf.float32)\n",
    "    with tf.name_scope('states'):\n",
    "        state_c = [[] for _ in range(len(opt_var))]\n",
    "        state_h = [[] for _ in range(len(opt_var))]\n",
    "        for i in range(len(opt_var)):\n",
    "            n_param = int(np.prod(shapes[i]))\n",
    "            state_c[i] = [tf.Variable(tf.zeros([n_param, hidden_size]), dtype=tf.float32, name=\"c_in\", trainable=False) for _ in range(num_layer)]\n",
    "            state_h[i] = [tf.Variable(tf.zeros([n_param, hidden_size]), dtype=tf.float32, name=\"h_in\", trainable=False) for _ in range(num_layer)]\n",
    "\n",
    "\n",
    "    def update_state(losstot, x, state_c, state_h):\n",
    "        with tf.name_scope(\"gradients\"):\n",
    "            shapes = [K.get_variable_shape(p) for p in x]\n",
    "            grads = K.gradients(losstot, x)\n",
    "            grads = [tf.stop_gradient(g) for g in grads]\n",
    "        with tf.variable_scope('MetaNetwork'):\n",
    "            cell_count = 0\n",
    "            delta = [[] for _ in range(len(grads))]\n",
    "            S_C_out = [[] for _ in range(len(opt_var))]\n",
    "            S_H_out = [[] for _ in range(len(opt_var))]\n",
    "            for i in range(len(grads)):\n",
    "                g = grads[i]\n",
    "                n_param = int(np.prod(shapes[i]))\n",
    "                flat_g = tf.reshape(g, [n_param, -1])\n",
    "                flat_g_mod = tf.reshape(log_encode(flat_g), [n_param, -1])\n",
    "                rnn_new_c = [[] for _ in range(num_layer)]\n",
    "                rnn_new_h = [[] for _ in range(num_layer)]\n",
    "            # Apply RNN cell for each parameter\n",
    "                with tf.variable_scope(\"RNN\"):\n",
    "                    rnn_outputs = []\n",
    "                    rnn_state_c = [[] for _ in range(num_layer)]\n",
    "                    rnn_state_h = [[] for _ in range(num_layer)]\n",
    "                    for ii in range(n_param):\n",
    "                        state_in = [tf.contrib.rnn.LSTMStateTuple(state_c[i][j][ii:ii + 1], state_h[i][j][ii:ii + 1]) for j in range(num_layer)]\n",
    "                        rnn_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(num_units=hidden_size, reuse=cell_count > 0) for _ in range(num_layer)])\n",
    "                        cell_count += 1\n",
    "\n",
    "                        # Verify whether the variables are used\n",
    "                        # for v in tf.global_variables():\n",
    "                        #     print(v.name)\n",
    "                    # Individual update with individual state but global cell params\n",
    "                        rnn_out_all, state_out = rnn_cell(flat_g_mod[ii:ii + 1, :], state_in)\n",
    "                        rnn_out = tf.add(tf.matmul(rnn_out_all, softmax_w), softmax_b)\n",
    "                        rnn_outputs.append(rnn_out)\n",
    "                        for j in range(num_layer):\n",
    "                            rnn_state_c[j].append(state_out[j].c)\n",
    "                            rnn_state_h[j].append(state_out[j].h)\n",
    "\n",
    "                    # Form output as tensor\n",
    "                rnn_outputs = tf.reshape(tf.stack(rnn_outputs, axis=1), g.get_shape())\n",
    "                for j in range(num_layer):\n",
    "                    rnn_new_c[j] = tf.reshape(tf.stack(rnn_state_c[j], axis=1), (n_param, hidden_size))\n",
    "                    rnn_new_h[j] = tf.reshape(tf.stack(rnn_state_h[j], axis=1), (n_param, hidden_size))\n",
    "\n",
    "            # Dense output from state\n",
    "                delta[i] = rnn_outputs\n",
    "                S_C_out[i] = rnn_new_c\n",
    "                S_H_out[i] = rnn_new_h\n",
    "\n",
    "        return delta, S_C_out, S_H_out\n",
    "\n",
    "    def time_step(t, f_array, x, state_c, state_h): \n",
    "        with tf.name_scope('Unroll_loss_t'):\n",
    "            fx = _make_with_custom_variables(loss, x)\n",
    "            f_array = f_array.write(t, fx)\n",
    "        with tf.name_scope('Unroll_delta_state_update'):\n",
    "            delta, s_c_out, s_h_out = update_state(fx, x, state_c, state_h)\n",
    "        with tf.name_scope('Unroll_Optimizee_update'):\n",
    "            x_new = [x_n + alpha*tf.tanh(d) for x_n, d in zip(x, delta)]\n",
    "        t_new = t + 1\n",
    "\n",
    "        return t_new, f_array, x_new, s_c_out, s_h_out\n",
    "\n",
    "    fx_array = tf.TensorArray(tf.float32, size=unroll_nn, clear_after_read=False)\n",
    "    _, fx_array, x_final, S_C, S_H = tf.while_loop(\n",
    "        cond=lambda t, *_: t < unroll_nn-1,\n",
    "        body=time_step,\n",
    "        loop_vars=(0, fx_array, opt_var, state_c, state_h),\n",
    "        parallel_iterations=1,\n",
    "        swap_memory=True,\n",
    "        name=\"unroll\")\n",
    "    with tf.name_scope('Unroll_loss_period'):\n",
    "        fx_final = _make_with_custom_variables(loss, x_final)\n",
    "        fx_array = fx_array.write(unroll_nn-1, fx_final)\n",
    "        arrayf = fx_array.stack()\n",
    "        \n",
    "    with tf.name_scope('Metaloss'):\n",
    "        loss_optimizer = tf.reduce_sum(fx_array.stack())\n",
    "\n",
    "    with tf.name_scope('MetaOpt'):\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    with tf.name_scope('Meta_update'):\n",
    "        step = optimizer.minimize(loss_optimizer)\n",
    "\n",
    "    with tf.name_scope('state_optimizee_var'):\n",
    "        variables = (nest.flatten(state_c) + nest.flatten(state_h) + opt_var)\n",
    "\n",
    "    with tf.name_scope('state_reset'):\n",
    "        reset = [tf.variables_initializer(variables), fx_array.close()]\n",
    "\n",
    "    with tf.name_scope('Optimizee_update'):\n",
    "        update = (nest.flatten([tf.assign(r, v) for r, v in zip(opt_var, x_final)]) +\n",
    "                    (nest.flatten([tf.assign(r, v) for r, v in zip(state_c[i], S_C[i]) for i in range(len(state_c))])) +\n",
    "                    (nest.flatten([tf.assign(r, v) for r, v in zip(state_h[i], S_H[i]) for i in range(len(state_h))])))\n",
    "    return step, loss_optimizer, update, reset, fx_final, arrayf, x_final\n",
    "\n",
    "def run_epoch(sess, num_iter, arraycost, cost_op, ops, reset):\n",
    "    sess.run(reset)\n",
    "    costepoch = []\n",
    "    for _ in range(num_iter):\n",
    "        cost, loss = [sess.run([arraycost, cost_op] + ops)[j] for j in range(2)]\n",
    "        costepoch.append(np.log10(cost))\n",
    "    return np.reshape(costepoch, -1), loss\n",
    "\n",
    "def print_stats(header, total_error_optimizee, total_time):\n",
    "    \"\"\"Prints experiment statistics.\"\"\"\n",
    "    print(header)\n",
    "    print(\"Mean Final Error Optimizee: {:.2f}\".format(total_error_optimizee))\n",
    "    print(\"Mean epoch time: {:.2f} s\".format(total_time))\n",
    "\n",
    "dictloss = problem(\"train\")\n",
    "step, loss_opt, update, reset, cost_op, arraycost, _ = metaopti(dictloss)\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    graph_writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    best_evaluation = float(\"inf\")\n",
    "    count = 0\n",
    "    start = timer()\n",
    "    num_iter = num_steps // unroll_nn\n",
    "    losstrain = []\n",
    "    losseval = []\n",
    "    plotlosstrain = []\n",
    "    plotlosseval = []\n",
    "    #Training\n",
    "    for e in range(n_epoch):\n",
    "        cost, trainloss = run_epoch(sess, num_iter, arraycost, cost_op, [step, update], reset)\n",
    "        losstrain.append(cost)\n",
    "        print_stats(\"Training Epoch {}\".format(e), trainloss, timer() - start)\n",
    "        saver = tf.train.Saver()\n",
    "        if (e + 1) % logging_period == 0:\n",
    "            plotlosstrain.append(cost)\n",
    "        if (e + 1) % evaluation_period == 0:\n",
    "            for _ in range(evaluation_epochs):\n",
    "                evalcost, evaloss = run_epoch(sess, num_iter, arraycost, cost_op, [update], reset)\n",
    "                losseval.append(evalcost)\n",
    "                if save_path is not None and evaloss < best_evaluation:\n",
    "                    print(\"Saving meta-optimizer to {}\".format(save_path))\n",
    "                    saver.save(sess, save_path, global_step=0)\n",
    "                    best_evaluation = evaloss\n",
    "                    plotlosseval.append(evalcost)\n",
    "    slengths = np.arange(num_steps)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(slengths, np.mean(plotlosstrain, 0), 'r-', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "    savefig('Training_MNIST.png')\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(slengths, np.mean(plotlosseval, 0), 'b-', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.legend()\n",
    "    savefig('Validation_MNIST.png')\n",
    "    plt.close()\n",
    "    graph_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
